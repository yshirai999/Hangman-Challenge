{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d073dc",
   "metadata": {},
   "source": [
    "# Affixes\n",
    "\n",
    "Visualization, general structures, statistics, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "381bff56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ed         31090\n",
      "  ng         21398\n",
      "  ati        21249\n",
      "  es         17210\n",
      "  tio        16296\n",
      "  nes        16224\n",
      "  ly         15704\n",
      "  er         15640\n",
      "  ter        15375\n",
      "  ica        12687\n",
      "  ic         12192\n",
      "  on         11856\n",
      "  ss         11622\n",
      "  al         11494\n",
      "  ent        10929\n",
      "  all        10764\n",
      "  tin        10596\n",
      "  eri        10479\n",
      "  le         10444\n",
      "  te         10196\n",
      "  us         9914\n",
      "  ate        9582\n",
      "  ver        9477\n",
      "  ali        9393\n",
      "  ing        8769\n",
      "  lin        8646\n",
      "  ion        8229\n",
      "  an         8128\n",
      "  sti        8046\n",
      "  per        7947\n",
      "  ell        7866\n",
      "  oni        7836\n",
      "  ist        7812\n",
      "  rat        7755\n",
      "  rin        7563\n",
      "  ene        7545\n",
      "  iti        7437\n",
      "  ari        7404\n",
      "  ine        7227\n",
      "  ere        7206\n",
      "  lat        7023\n",
      "  ill        7008\n",
      "  tic        6993\n",
      "  atio       6974\n",
      "  tra        6873\n",
      "  ran        6834\n",
      "  lit        6816\n",
      "  era        6684\n",
      "  ste        6678\n",
      "  st         6522\n",
      "  ili        6372\n",
      "  men        6351\n",
      "  the        6351\n",
      "  abl        6256\n",
      "  olo        6234\n",
      "  ona        6084\n",
      "  res        6006\n",
      "  ero        5994\n",
      "  ne         5974\n",
      "  nte        5950\n",
      "  rs         5870\n",
      "  ant        5856\n",
      "  ato        5826\n",
      "  nt         5788\n",
      "  tri        5772\n",
      "  cal        5763\n",
      "  est        5712\n",
      "  ia         5710\n",
      "  ani        5664\n",
      "  ast        5658\n",
      "  ect        5631\n",
      "  min        5628\n",
      "  oph        5625\n",
      "  nti        5546\n",
      "  and        5541\n",
      "  str        5514\n",
      "  der        5511\n",
      "  mat        5508\n",
      "  ess        5391\n",
      "  nde        5318\n",
      "  tro        5304\n",
      "  emi        5289\n",
      "  ori        5286\n",
      "  log        5262\n",
      "  nis        5226\n",
      "  ula        5169\n",
      "  gra        5157\n",
      "  ina        5148\n",
      "  nat        5136\n",
      "  ous        5121\n",
      "  ers        5109\n",
      "  rop        5097\n",
      "  lis        4983\n",
      "  her        4962\n",
      "  tor        4941\n",
      "  eli        4932\n",
      "  ini        4827\n",
      "  con        4767\n",
      "  chi        4725\n",
      "  one        4722\n"
     ]
    }
   ],
   "source": [
    "from utils import load_cache\n",
    "\n",
    "def preview_affix_data(affix_data, top_n=15):\n",
    "    for k, v in sorted(affix_data.items(), key=lambda x: -x[1])[:top_n]:\n",
    "        print(f\"  {k:<10} {v}\")\n",
    "\n",
    "affix_data = load_cache(\"affixes_train_cons.pkl\")\n",
    "preview_affix_data(affix_data=affix_data, top_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279cfed",
   "metadata": {},
   "source": [
    "Minimal affixes length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95f40dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(min([len(k) for k in affix_data.keys()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f51fdff",
   "metadata": {},
   "source": [
    "Structure of the affixes file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f77e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One sample from affixes:\n",
      "th: 1268\n",
      "thi: 2463\n",
      "no: 510\n",
      "in: 4010\n",
      "int: 3180\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "\n",
    "with open(\"affixes_train_cons.pkl\", \"rb\") as f:\n",
    "    affixes = load(f)\n",
    "\n",
    "# Just to confirm the structure:\n",
    "print(\"One sample from affixes:\")\n",
    "for k, v in list(affixes.items())[:5]:\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfe990",
   "metadata": {},
   "source": [
    "# Analysis of Subpattern Gradient agent\n",
    "\n",
    "The SB agent tends to recover at least 60% of the letters in about 60-70% of the cases for words of length between 5-10 where it fails to reveal the full answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aaf2345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¢ Total samples: 1000\n",
      "âœ… Samples â‰¥ 60% revealed: 702 / 1000 (70.20%)\n",
      "\n",
      "ðŸ“Š Average reveal ratio by word length:\n",
      " - Length  6: 60.11% over 155 samples\n",
      " - Length  7: 68.81% over 180 samples\n",
      " - Length  8: 66.55% over 210 samples\n",
      " - Length  9: 69.27% over 239 samples\n",
      " - Length 10: 72.18% over 216 samples\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "def count_blanks(pattern):\n",
    "    return sum(1 for c in pattern if c == \"_\")\n",
    "\n",
    "# Load data\n",
    "with open(\"transformer_eval_data_from_subpattern_short.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(f\"ðŸ”¢ Total samples: {len(data)}\")\n",
    "\n",
    "# Validate each sample's reveal %\n",
    "reveal_stats = []\n",
    "length_buckets = defaultdict(list)\n",
    "\n",
    "for pattern, guessed, true_word in data:\n",
    "    word_len = len(true_word)\n",
    "    blanks = count_blanks(pattern)\n",
    "    reveal_ratio = (word_len - blanks) / word_len\n",
    "    reveal_stats.append(reveal_ratio)\n",
    "    length_buckets[word_len].append(reveal_ratio)\n",
    "\n",
    "# Summary\n",
    "above_60 = sum(r >= 0.6 for r in reveal_stats)\n",
    "print(f\"âœ… Samples â‰¥ 60% revealed: {above_60} / {len(data)} ({above_60 / len(data):.2%})\")\n",
    "\n",
    "# Optional: average by length\n",
    "print(\"\\nðŸ“Š Average reveal ratio by word length:\")\n",
    "for length in sorted(length_buckets):\n",
    "    ratios = length_buckets[length]\n",
    "    avg = sum(ratios) / len(ratios)\n",
    "    print(f\" - Length {length:2d}: {avg:.2%} over {len(ratios)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26e5460f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      " - Pattern:    moreo_er\n",
      " - Guessed:    aeimnort\n",
      " - True word:  moreover\n",
      "\n",
      "Sample 2:\n",
      " - Pattern:    _a_ead_\n",
      " - Guessed:    acdehirt\n",
      " - True word:  makeado\n",
      "\n",
      "Sample 3:\n",
      " - Pattern:    scaril_\n",
      " - Guessed:    acehilorst\n",
      " - True word:  scarily\n",
      "\n",
      "Sample 4:\n",
      " - Pattern:    _atrolmen\n",
      " - Guessed:    aceilmnort\n",
      " - True word:  patrolmen\n",
      "\n",
      "Sample 5:\n",
      " - Pattern:    __errar_\n",
      " - Guessed:    aeirt\n",
      " - True word:  sherrard\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (pattern, guessed, true_word) in enumerate(data[:5]):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\" - Pattern:    {pattern}\")\n",
    "    print(f\" - Guessed:    {guessed}\")\n",
    "    print(f\" - True word:  {true_word}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9cc742a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total full-history games: 25000\n",
      "First full-history sample:\n",
      "('e_erting', 'aceginrst', 'everting')\n",
      "('__l_ie', 'defilrt', 'walkie')\n",
      "('o_t_ta_t', 'aeinot', 'outstart')\n",
      "('neo_esha', 'aehilmnorstv', 'neodesha')\n",
      "('__allic', 'aceilnrst', 'phallic')\n",
      "('_appers', 'aehinoprstv', 'lappers')\n",
      "('pi___er', 'adeioprtv', 'pincher')\n",
      "('_ri_e__s', 'deinrsvz', 'writeups')\n",
      "('l_s_inia', 'ahilmnrst', 'luscinia')\n",
      "('_oori__', 'aeinor', 'boorish')\n",
      "('tar_e_', 'acdeimort', 'tarbes')\n",
      "('_e_ri_', 'aeinrt', 'bedrid')\n",
      "('_aters', 'aeilmorst', 'haters')\n",
      "('t__e_ton', 'aceinopst', 'tylerton')\n",
      "('r__tee', 'aehiortw', 'runtee')\n",
      "('_enti_at__', 'aceint', 'ventilator')\n",
      "('_iver', 'einrtv', 'liver')\n",
      "('_raman', 'acegimnrt', 'braman')\n",
      "('turtle_e_', 'abeilnortu', 'turtlepeg')\n",
      "('__a_le', 'aeilnrt', 'chawle')\n"
     ]
    }
   ],
   "source": [
    "with open(\"transformer_data_from_subpattern_short25000.pkl\", \"rb\") as f:\n",
    "    full_history = pickle.load(f)\n",
    "\n",
    "print(f\"Total full-history games: {len(full_history)}\")\n",
    "print(\"First full-history sample:\")\n",
    "for step in full_history[:20]:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "056e658a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total full-history games: 1000\n",
      "First full-history sample:\n",
      "('moreo_er', 'aeimnort', 'moreover')\n",
      "('_a_ead_', 'acdehirt', 'makeado')\n",
      "('scaril_', 'acehilorst', 'scarily')\n",
      "('_atrolmen', 'aceilmnort', 'patrolmen')\n",
      "('__errar_', 'aeirt', 'sherrard')\n",
      "('_lan_il_', 'acdeilnrtu', 'slangily')\n",
      "('uproot_', 'aehinoprtu', 'uproots')\n",
      "('e_ocardiac', 'acdehilmnort', 'exocardiac')\n",
      "('r__i_alia', 'acdeilnrt', 'robigalia')\n",
      "('t_rannized', 'adeinorstuvz', 'tyrannized')\n",
      "('ee_i__', 'eir', 'eetion')\n",
      "('__i_alian', 'acdeilnrtz', 'phigalian')\n",
      "('sn___ered', 'deinprstv', 'snookered')\n",
      "('t__iad', 'acdeilnrt', 'thyiad')\n",
      "('e__o_t', 'aeiorst', 'excoct')\n",
      "('rea___stom', 'adeimorst', 'reaccustom')\n",
      "('_live__le', 'eilnrv', 'olivepale')\n",
      "('aard_o__', 'adehiort', 'aardwolf')\n",
      "('__ori_', 'aeinor', 'florid')\n",
      "('_res___it', 'aeilnorst', 'presubmit')\n"
     ]
    }
   ],
   "source": [
    "with open(\"transformer_eval_data_from_subpattern_short.pkl\", \"rb\") as f:\n",
    "    full_history = pickle.load(f)\n",
    "\n",
    "print(f\"Total full-history games: {len(full_history)}\")\n",
    "print(\"First full-history sample:\")\n",
    "for step in full_history[:20]:\n",
    "    print(step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48272845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model max sequence length set to: 11\n",
      "Transformer model loaded from checkpoints/v3_run/transformer_best.pt\n",
      "[âœ“] Loaded 1000 samples from transformer_eval_data_from_subpattern_short.pkl\n",
      "\n",
      "[Sample 1]\n",
      "Pattern: moreo_er\n",
      "Guessed: aeimnort\n",
      "True:    moreover\n",
      "Predicted letter: g\n",
      "Blank Acc: 0.00% (0/1)\n",
      "Prediction based on transformer_prediction(): g\n",
      "\n",
      "[Sample 2]\n",
      "Pattern: _a_ead_\n",
      "Guessed: acdehirt\n",
      "True:    makeado\n",
      "Predicted letter: l\n",
      "Blank Acc: 0.00% (0/3)\n",
      "Prediction based on transformer_prediction(): l\n",
      "\n",
      "[Sample 3]\n",
      "Pattern: scaril_\n",
      "Guessed: acehilorst\n",
      "True:    scarily\n",
      "Predicted letter: y\n",
      "Blank Acc: 100.00% (1/1)\n",
      "Prediction based on transformer_prediction(): y\n",
      "\n",
      "[Sample 4]\n",
      "Pattern: _atrolmen\n",
      "Guessed: aceilmnort\n",
      "True:    patrolmen\n",
      "Predicted letter: p\n",
      "Blank Acc: 100.00% (1/1)\n",
      "Prediction based on transformer_prediction(): p\n",
      "\n",
      "[Sample 5]\n",
      "Pattern: __errar_\n",
      "Guessed: aeirt\n",
      "True:    sherrard\n",
      "Predicted letter: v\n",
      "Blank Acc: 0.00% (0/3)\n",
      "Prediction based on transformer_prediction(): v\n",
      "\n",
      "[Sample 6]\n",
      "Pattern: _lan_il_\n",
      "Guessed: acdeilnrtu\n",
      "True:    slangily\n",
      "Predicted letter: y\n",
      "Blank Acc: 33.33% (1/3)\n",
      "Prediction based on transformer_prediction(): y\n",
      "\n",
      "[Sample 7]\n",
      "Pattern: uproot_\n",
      "Guessed: aehinoprtu\n",
      "True:    uproots\n",
      "Predicted letter: s\n",
      "Blank Acc: 100.00% (1/1)\n",
      "Prediction based on transformer_prediction(): s\n",
      "\n",
      "[Sample 8]\n",
      "Pattern: e_ocardiac\n",
      "Guessed: acdehilmnort\n",
      "True:    exocardiac\n",
      "Predicted letter: s\n",
      "Blank Acc: 0.00% (0/1)\n",
      "Prediction based on transformer_prediction(): s\n",
      "\n",
      "[Sample 9]\n",
      "Pattern: r__i_alia\n",
      "Guessed: acdeilnrt\n",
      "True:    robigalia\n",
      "Predicted letter: h\n",
      "Blank Acc: 0.00% (0/3)\n",
      "Prediction based on transformer_prediction(): h\n",
      "\n",
      "[Sample 10]\n",
      "Pattern: t_rannized\n",
      "Guessed: adeinorstuvz\n",
      "True:    tyrannized\n",
      "Predicted letter: y\n",
      "Blank Acc: 100.00% (1/1)\n",
      "Prediction based on transformer_prediction(): y\n",
      "Evaluation completed for checkpoints/v3_run/transformer_best.pt.\n"
     ]
    }
   ],
   "source": [
    "## Testing transformer_prediction is consistent with evaluate_prediction function\n",
    "\n",
    "from submission_agent import agent\n",
    "from agent import HangmanAgent\n",
    "from utils import HangmanTransformerModelV2, HangmanTransformerModelV3\n",
    "\n",
    "if agent.mode != \"Transformer\":\n",
    "    print(f\"Current Agent mode: {agent.mode}\")\n",
    "    print(\"Switching to Transformer mode...\")\n",
    "    agent.mode = \"Transformer\"\n",
    "    print(f\"Switched Agent mode to: {agent.mode}\")\n",
    "\n",
    "# Ensure model is loaded and on GPU and run evaluation\n",
    "if agent.TransformerModel==HangmanTransformerModelV3:\n",
    "    model_path = \"checkpoints/v3_run/transformer_best.pt\"\n",
    "    agent.transformer_model = agent.load_transformer_model(\n",
    "        HangmanTransformerModelV3,\n",
    "        model_path,\n",
    "        use_cuda=True,\n",
    "    )\n",
    "    agent.evaluate_transformer_prediction(\n",
    "        use_blank_mask_for_transformer=True,\n",
    "        eval_data_path=\"transformer_eval_data_from_subpattern_short.pkl\",\n",
    "        log_path=\"logs/v3_eval/transformer_best.jsonl\",\n",
    "        n_eval = 10\n",
    "    )\n",
    "elif agent.TransformerModel==HangmanTransformerModelV2:\n",
    "    print(\"Using HangmanTransformerModelV2\")\n",
    "    model_path = \"checkpoints/v2_run/transformer_best.pt\"\n",
    "    agent.transformer_model = agent.load_transformer_model(\n",
    "        HangmanTransformerModelV2,\n",
    "        model_path,\n",
    "        use_cuda=True\n",
    "    )\n",
    "    agent.evaluate_transformer_prediction(\n",
    "        eval_data_path=\"transformer_eval_data_from_subpattern.pkl\"\n",
    "    )\n",
    "\n",
    "print(f\"Evaluation completed for {model_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hangman-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
